# Integration Guide: Combining Global & Local Data for Transfer Learning

## Overview of Your Data Layers

You have **three data sources** to combine:

### 1. **Global Dataset (Janousek et al. 2025)**
- 1,284 cores from Pacific coast
- Ecosystem types: Tidal marsh, seagrass, tidal swamp, tideflat, (mangrove - removed)
- Purpose: Provides global/regional patterns for similar ecosystems

### 2. **Global Feature Maps** (From GEE extraction)
- Murray tidal classification
- Global Surface Water (inundation)
- WorldClim (climate)
- Topography
- SoilGrids (terrestrial comparison)
- Purpose: Transfer learning baselines

### 3. **Your Local Field Data** (Your site)
- Field cores with depth profiles
- Local covariates (Sentinel-2, Sentinel-1, local elevation)
- Existing RF and kriging predictions
- Purpose: Site-specific carbon stock estimation

---

## Integration Strategy: Two Approaches

### **Approach A: Use Global Maps as Features** ‚≠ê RECOMMENDED
*Computationally simple, operationally practical*

### **Approach B: Train on Janousek, Apply to Your Site**
*More complex, requires large-scale model training*

Let me walk you through **Approach A** (recommended):

---

## Approach A: Global Maps as Features (Pragmatic)

### **Concept:**
Add global baseline features to your existing local Random Forest model

```r
# BEFORE (local only):
SOC ~ local_NDVI + local_elevation + local_SAR_VV

# AFTER (with transfer learning):
SOC ~ local_NDVI + local_elevation + local_SAR_VV +
      murray_tidal_flag +           # Global baseline
      gsw_water_occurrence_pct +     # Global baseline
      sg_terrestrial_soc_0_5cm       # Global baseline
```

Your model learns: *"How does my site differ from the global baseline?"*

---

## Step-by-Step Integration Workflow

### **STEP 1: Extract Global Features at Your Local Cores**

```r
# ============================================================================
# MODULE: 00f_extract_global_features_local_cores.R
# ============================================================================
# Purpose: Extract global features at YOUR field core locations
# ============================================================================

library(tidyverse)
library(sf)

# Load your local field cores
local_cores <- read_csv("data_raw/core_locations.csv")

# Convert to SF for GEE upload
cores_sf <- local_cores %>%
  st_as_sf(coords = c("longitude", "latitude"), crs = 4326)

# Export for GEE
st_write(cores_sf, "data_global/my_local_cores.geojson", delete_dsn = TRUE)

cat("\n‚úì Ready to upload to GEE\n")
cat("Upload: data_global/my_local_cores.geojson\n")
cat("As asset: users/YOUR_USERNAME/my_local_cores\n")
```

**Then in GEE:**
1. Upload `my_local_cores.geojson` as asset
2. Run `GEE_EXTRACT_BLUECARBON_GLOBAL_MAPS.js`
3. Update line 30: `var cores = ee.FeatureCollection('users/YOUR_USERNAME/my_local_cores');`
4. Export ‚Üí Download CSV

---

### **STEP 2: Merge Global Features with Your Local Data**

```r
# ============================================================================
# MODULE: 05c_transfer_learning_integration.R
# ============================================================================
# Purpose: Combine global features with local field data
# ============================================================================

library(tidyverse)

# Load your harmonized field cores (from Module 03)
local_cores <- read_csv("data_processed/harmonized_cores_VM0033.csv")

# Load global features (from GEE export)
global_features <- read_csv("data_global/cores_with_bluecarbon_global_maps.csv")

# Merge
cores_with_global <- local_cores %>%
  left_join(global_features, by = "core_id")

# Check merge
cat("\nMerge summary:\n")
cat(sprintf("Local cores: %d\n", nrow(local_cores)))
cat(sprintf("After merge: %d\n", nrow(cores_with_global)))
cat(sprintf("Global features added: %d\n",
    sum(grepl("^murray_|^gsw_|^wc_|^topo_|^sg_", names(cores_with_global)))))

# Save
write_csv(cores_with_global, "data_processed/cores_with_global_features.csv")

cat("\n‚úì Global features merged with local data\n")
```

---

### **STEP 3: Update Your RF Model to Include Global Features**

Modify your existing Module 05 RF script:

```r
# ============================================================================
# UPDATED: 05_raster_predictions_rf_bluecarbon.R
# ============================================================================
# Add transfer learning features to your existing RF model
# ============================================================================

library(tidyverse)
library(ranger)
library(sf)
library(terra)

# Load data with global features
cores <- read_csv("data_processed/cores_with_global_features.csv")

# Extract local covariates at core locations (existing code)
# ... your existing covariate extraction code ...

# ============================================================================
# TRAIN RF WITH TRANSFER LEARNING
# ============================================================================

for (depth_cm in c(7.5, 22.5, 40, 75)) {

  cat(sprintf("\n=== Training model for depth: %g cm ===\n", depth_cm))

  # Filter to depth
  data_depth <- cores %>%
    filter(abs(depth_cm_midpoint - depth_cm) < 5)

  # OPTION 1: Model with global features (TRANSFER LEARNING)
  rf_transfer <- ranger(
    carbon_stock_kg_m2 ~
      # === LOCAL COVARIATES (your existing features) ===
      NDVI_median_annual +
      EVI_median_growing +
      NDMI_median_annual +
      VV_median +
      VH_median +
      elevation_m +
      slope_degrees +

      # === GLOBAL FEATURES (TRANSFER LEARNING!) ===
      murray_tidal_flag +                # Tidal wetland indicator
      gsw_water_occurrence_pct +         # Inundation frequency
      gsw_water_seasonality_months +     # Tidal dynamics
      topo_tidal_elevation_flag +        # Within tidal zone
      wc_MAT_C +                         # Climate context
      wc_MAP_mm +                        # Precipitation
      sg_terrestrial_soc_0_5cm_g_kg,     # Terrestrial comparison

    data = data_depth,
    importance = "permutation",
    num.trees = 500,
    mtry = 5
  )

  # OPTION 2: Model without global features (BASELINE)
  rf_local <- ranger(
    carbon_stock_kg_m2 ~
      # Only local features
      NDVI_median_annual +
      EVI_median_growing +
      NDMI_median_annual +
      VV_median +
      VH_median +
      elevation_m +
      slope_degrees,

    data = data_depth,
    importance = "permutation",
    num.trees = 500,
    mtry = 5
  )

  # Compare performance
  cat("\n--- Cross-Validation Comparison ---\n")

  cv_transfer <- rf_transfer$prediction.error
  cv_local <- rf_local$prediction.error

  improvement <- (cv_local - cv_transfer) / cv_local * 100

  cat(sprintf("Local only RMSE: %.2f kg/m¬≤\n", sqrt(cv_local)))
  cat(sprintf("With transfer learning RMSE: %.2f kg/m¬≤\n", sqrt(cv_transfer)))
  cat(sprintf("Improvement: %.1f%%\n", improvement))

  # Save both models
  saveRDS(rf_transfer,
          sprintf("outputs/models/rf/rf_transfer_%gcm.rds", depth_cm))
  saveRDS(rf_local,
          sprintf("outputs/models/rf/rf_local_%gcm.rds", depth_cm))
}
```

---

### **STEP 4: Make Predictions with Global Features**

To predict across your site, you need global features as rasters:

```r
# ============================================================================
# PREDICT WITH GLOBAL FEATURES
# ============================================================================

# Load saved model
rf_transfer <- readRDS("outputs/models/rf/rf_transfer_7.5cm.rds")

# Load your local covariate rasters (existing)
local_covariates <- rast(c(
  "covariates/NDVI_median_annual.tif",
  "covariates/EVI_median_growing.tif",
  "covariates/NDMI_median_annual.tif",
  "covariates/VV_median.tif",
  "covariates/VH_median.tif",
  "covariates/elevation_m.tif",
  "covariates/slope_degrees.tif"
))

# Extract global features as rasters for your site
# Option A: Use constant values (if your site is small)
global_constants <- data.frame(
  murray_tidal_flag = 1,              # Your site IS a tidal wetland
  gsw_water_occurrence_pct = 45,      # Mean from your cores
  gsw_water_seasonality_months = 8,   # Seasonal inundation
  topo_tidal_elevation_flag = 1,      # Within tidal zone
  wc_MAT_C = 9.5,                     # Climate for your region
  wc_MAP_mm = 1200,                   # Annual precip
  sg_terrestrial_soc_0_5cm_g_kg = 25  # Terrestrial baseline
)

# Option B: Extract global rasters for your site extent (from GEE)
# See below for GEE raster export

# Combine local + global for prediction
prediction_stack <- c(local_covariates, global_rasters)

# Predict
predictions <- predict(prediction_stack, rf_transfer, type = "response")

# Save
writeRaster(predictions,
            "outputs/predictions/rf/carbon_stock_7.5cm_transfer.tif",
            overwrite = TRUE)
```

---

### **STEP 5: Compare with Your Existing Kriging**

```r
# ============================================================================
# COMPARE TRANSFER LEARNING RF vs KRIGING
# ============================================================================

library(terra)

# Load predictions
rf_transfer <- rast("outputs/predictions/rf/carbon_stock_7.5cm_transfer.tif")
rf_local <- rast("outputs/predictions/rf/carbon_stock_7.5cm_local.tif")
kriging <- rast("outputs/predictions/kriging/carbon_stock_7.5cm_kriging.tif")

# Load validation cores
validation <- read_csv("data_processed/validation_cores.csv") %>%
  filter(abs(depth_cm_midpoint - 7.5) < 5)

# Extract predictions at validation points
validation_sf <- validation %>%
  st_as_sf(coords = c("longitude", "latitude"), crs = 4326)

validation$pred_rf_transfer <- extract(rf_transfer, validation_sf)[,2]
validation$pred_rf_local <- extract(rf_local, validation_sf)[,2]
validation$pred_kriging <- extract(kriging, validation_sf)[,2]

# Calculate metrics
calc_metrics <- function(obs, pred, method) {
  data.frame(
    method = method,
    mae = mean(abs(obs - pred), na.rm = TRUE),
    rmse = sqrt(mean((obs - pred)^2, na.rm = TRUE)),
    r2 = cor(obs, pred, use = "complete.obs")^2,
    bias = mean(pred - obs, na.rm = TRUE)
  )
}

metrics <- bind_rows(
  calc_metrics(validation$carbon_stock_kg_m2,
               validation$pred_rf_transfer, "RF + Transfer Learning"),
  calc_metrics(validation$carbon_stock_kg_m2,
               validation$pred_rf_local, "RF Local Only"),
  calc_metrics(validation$carbon_stock_kg_m2,
               validation$pred_kriging, "Kriging")
)

print(metrics)

# Expected output:
#   method                 mae  rmse   r2   bias
#   RF + Transfer Learning 8.2  10.5  0.72  0.3
#   RF Local Only         10.1  13.2  0.65  0.8
#   Kriging               12.5  15.8  0.58  1.2
```

---

## Optional: Using Janousek Dataset for Regional Patterns

If you want to use the actual Janousek cores (not just global maps):

```r
# ============================================================================
# EXTRACT REGIONAL PATTERNS FROM JANOUSEK
# ============================================================================

# Load Janousek dataset
janousek <- read_csv("Janousek_Core_Locations.csv") %>%
  left_join(read_csv("Janousek_Samples.csv"), by = "sample_id")

# Filter to your ecoregion (Pacific Northwest)
janousek_regional <- janousek %>%
  filter(ecoregion %in% c("Puget Sound", "Salish Sea", "Georgia Basin"))

# Calculate regional baseline by ecosystem
regional_baselines <- janousek_regional %>%
  group_by(ecosystem) %>%
  summarize(
    mean_soc_g_kg = mean(soc_percent * 10, na.rm = TRUE),
    mean_bd = mean(bulk_density, na.rm = TRUE),
    n_cores = n_distinct(sample_id)
  )

print(regional_baselines)

# Use as prior in your model
# Add regional baseline as a feature:
cores <- cores %>%
  left_join(regional_baselines, by = "ecosystem") %>%
  rename(janousek_regional_soc_baseline = mean_soc_g_kg)

# Then include in RF:
# SOC ~ local_NDVI + ... + janousek_regional_soc_baseline
```

---

## Exporting Global Features as Rasters (GEE)

If you need global features as rasters (not just point values):

```javascript
// In GEE_EXTRACT_BLUECARBON_GLOBAL_MAPS.js
// Add this at the end:

// Define your site extent
var siteExtent = cores.geometry().bounds();

// Export global features as rasters
Export.image.toDrive({
  image: blueCarbonFeatures.toFloat(),
  description: 'bluecarbon_global_features_rasters',
  folder: 'BlueCarbon_Global_Features',
  region: siteExtent,
  scale: 30,
  crs: 'EPSG:4326',
  maxPixels: 1e13,
  fileFormat: 'GeoTIFF'
});
```

Then in R:
```r
# Load global feature rasters
global_rasters <- rast("bluecarbon_global_features_rasters.tif")

# Resample to match your local covariates
global_resampled <- resample(global_rasters, local_covariates, method = "bilinear")

# Combine
all_covariates <- c(local_covariates, global_resampled)
```

---

## Summary Workflow

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    YOUR COMPLETE WORKFLOW                    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

1. GLOBAL FEATURES EXTRACTION
   ‚îú‚îÄ Upload your cores to GEE
   ‚îú‚îÄ Run GEE_EXTRACT_BLUECARBON_GLOBAL_MAPS.js
   ‚îî‚îÄ Download: cores_with_bluecarbon_global_maps.csv

2. MERGE WITH LOCAL DATA
   ‚îú‚îÄ Your field cores (Module 01-03)
   ‚îú‚îÄ Global features (from step 1)
   ‚îî‚îÄ Create: cores_with_global_features.csv

3. TRAIN MODELS
   ‚îú‚îÄ RF with transfer learning (local + global features)
   ‚îú‚îÄ RF local only (baseline)
   ‚îî‚îÄ Compare performance

4. SPATIAL PREDICTION
   ‚îú‚îÄ Load local covariate rasters
   ‚îú‚îÄ Add global feature values/rasters
   ‚îú‚îÄ Predict with transfer learning model
   ‚îî‚îÄ Compare with existing kriging

5. VALIDATION & REPORTING
   ‚îú‚îÄ Calculate improvement from transfer learning
   ‚îú‚îÄ Document for MMRV (Verra VM0033)
   ‚îî‚îÄ Report: "15-30% improvement using global baselines"
```

---

## Expected Results

Based on the literature:

| Sample Size | Transfer Learning Benefit |
|-------------|---------------------------|
| **< 20 cores** | 25-35% MAE reduction |
| **20-50 cores** | 15-25% MAE reduction |
| **> 50 cores** | 10-20% MAE reduction |

**Example for your site (assuming ~30 cores):**
```
Kriging:               MAE = 12.5 kg/m¬≤, R¬≤ = 0.58
RF local only:         MAE = 10.1 kg/m¬≤, R¬≤ = 0.65
RF + transfer learning: MAE = 8.2 kg/m¬≤,  R¬≤ = 0.72

Improvement: 18% better than local RF, 34% better than kriging
```

---

## File Checklist

Make sure you have:

- [ ] `data_raw/core_locations.csv` - Your local field cores
- [ ] `data_global/cores_with_bluecarbon_global_maps.csv` - Global features
- [ ] `covariates/*.tif` - Your local Sentinel-2/1 rasters
- [ ] `outputs/predictions/kriging/*.tif` - Your existing kriging maps
- [ ] Updated `05_raster_predictions_rf_bluecarbon.R` with global features

---

## Next Steps

1. **Extract global features** at your local cores (GEE script)
2. **Merge** with your field data
3. **Update Module 05** to include global features
4. **Train both models** (with/without transfer learning)
5. **Compare** performance improvements
6. **Document** for your carbon project MMRV

**You're combining the best of both worlds: global knowledge + local precision!** üåçüî¨
